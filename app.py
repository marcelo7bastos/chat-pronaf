# ‚îÄ‚îÄ‚îÄ compatibilidade SQLite / Chroma ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
import sys, importlib, platform

try:
    if platform.system() != "Windows":          # Linux, macOS, Cloud‚Ä¶
        import pysqlite3                        # wheel ‚â• 0.5.3
        sys.modules["sqlite3"] = sys.modules.pop("pysqlite3")
        importlib.invalidate_caches()
except ModuleNotFoundError:
    # Estamos no Windows (ou o wheel n√£o foi instalado). 
    # Continuamos com o sqlite3 da stdlib, que j√° √© ‚â• 3.41 no Python 3.12.
    pass
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


import streamlit as st

# üéØ CONFIGURA√á√ïES INICIAIS DA P√ÅGINA - PRIMEIRO comando Streamlit
st.set_page_config(
    page_title="Chatbot PRONAF",
    page_icon="ü§ñ",
    layout="wide"
)

import pandas as pd
import openai
import json

# üß† CONFIGURA√á√ÉO DA CHAVE DE API
# Use st.secrets para seguran√ßa no Community Cloud (ou carregue de vari√°vel de ambiente local)
#from dotenv import load_dotenv
import os

# Importa√ß√µes para o vector store e recupera√ß√£o
# from langchain_community.vectorstores import Chroma 
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader  # se necess√°rio carregar novos documentos
#from langchain_chroma import Chroma
from langchain_community.vectorstores import Chroma



# Importa√ß√µes para RAG e prompt
from langchain import hub
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser


#load_dotenv()  # Carrega vari√°veis do .env

openai.api_key = st.secrets["openai_api_key"]


######### C√≥digo de carregamento do vector store, RAG e fun√ß√µes do RAG #########
# üóÇÔ∏è CARREGAMENTO DO VETOR STORE
# Configurar os embeddings com o modelo escolhido
# embedding_engine = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2") # melhor modelo, mas mais pesado
#embedding_engine = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2") # modelo mais leve e r√°pido, mas menos preciso

# Modelo encapsulado em fun√ß√£o para facilitar o cache
@st.cache_resource(show_spinner="Carregando embeddings‚Ä¶")
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

embedding_engine = load_embeddings()


# Carregar o vector store persistido
try:
    vector_db = Chroma(
        persist_directory=r"data\data-rag\persist_directory", 
        embedding_function=embedding_engine
    )
except Exception as e:
    raise RuntimeError(f"Erro ao carregar o vector_db: {e}")

# Verificar se o vector_db foi carregado corretamente
if vector_db is None:
    raise RuntimeError("O vector_db n√£o foi carregado corretamente. Verifique o caminho e os dados.")

# Definir o n√∫mero de documentos a serem recuperados
n_documentos = 3


#### Fun√ß√£o para formatar os documentos recuperados para o RAG
def format_docs(documentos):
    """
    Concatena os conte√∫dos dos documentos em um √∫nico texto com separador de duas quebras de linha.
    Adicionalmente, inclui um cabe√ßalho com metadados relevantes (por exemplo, n√∫mero da p√°gina), se dispon√≠veis.

    Args:
        documentos (list): Lista de objetos que possuem os atributos 'page_content' e, opcionalmente, 'metadata'.

    Returns:
        str: Texto concatenado dos conte√∫dos dos documentos que possuem conte√∫do v√°lido.
    """
    formatted_texts = []  # Lista para armazenar os textos formatados

    for doc in documentos:
        # Verifica se o objeto possui o atributo 'page_content'
        if hasattr(doc, "page_content"):
            content = doc.page_content.strip()  # Remove espa√ßos extras do in√≠cio e fim
            # Se o conte√∫do n√£o estiver vazio
            if content:
                header = ""
                # Se existirem metadados e, em particular, o n√∫mero da p√°gina estiver dispon√≠vel, insere o cabe√ßalho
                if hasattr(doc, "metadata") and doc.metadata.get("page"):
                    header = f"P√°gina: {doc.metadata.get('page')}\n"
                formatted_texts.append(f"{header}{content}")

    # Junta os textos formatados usando duas quebras de linha como separador
    return "\n\n".join(formatted_texts)
##############





# üß† T√≠tulo e Introdu√ß√£o
st.title("üìä Chatbot PRONAF")
st.write("Interaja com os dados do Programa Nacional de Fortalecimento da Agricultura Familiar (PRONAF).")
st.markdown("Aqui voc√™ pode consultar informa√ß√µes sobre o PRONAF e as linhas de cr√©dito para o Agricultor Famliar" \
            "e, tamb√©m, sobre cr√©dito agr√≠cola por **estado**, **sexo** e **ano**.")

# ‚ÑπÔ∏è Instru√ß√µes
st.markdown("##### ‚ÑπÔ∏è Alguns exemplos:")
st.markdown("""
- üßÆ *"Qual o valor total de cr√©dito para o estado SP?"*
- üóìÔ∏è *"Quantas opera√ß√µes foram realizadas no RS em 2024?"*
- üë©‚Äçüåæ *"Quantas agricultoras mulheres houve em MG em 2025?"*
- üìä *"Qual o total de cr√©dito disponibilizado para homens e mulheres em Minas Gerais no ano de 2024?"*
- üí∞ *"Valor total de cr√©dito concedido a agricultoras no Paran√° em 2024?"*
""")
st.markdown("---")



# üì¶ CARREGAMENTO DO DATASET COM CACHE
@st.cache_data
def carregar_dados():
    #df = pd.read_csv("data/pronaf.csv")
    df = pd.read_parquet("data/pronaf.parquet")
    return df

# üîß FUN√á√ÉO LOCAL PARA CONSULTAR O DATASET
def consulta_pronaf_por_estado(cd_estado: str) -> str:
    """
    Filtra os dados do PRONAF por estado e retorna um resumo em Markdown.
    """
    df = carregar_dados()
    df_estado = df[df["CD_ESTADO"] == cd_estado.upper()]
    if df_estado.empty:
        return f"Nenhum dado encontrado para o estado '{cd_estado}'. Verifique o c√≥digo UF."
    
    resumo = df_estado.groupby(["ANO", "SEXO_BIOLOGICO"]).agg({
        "VL_PARC_CREDITO": "sum",
        "CD_CPF_CNPJ": ["count", "nunique"]
    }).reset_index()
    
    resumo.columns = ["ANO", "SEXO_BIOLOGICO", "Soma_VL_PARC_CREDITO", "Quantidade_Operacoes", "Quantidade_Beneficiarios"]
    
    tabela = resumo.to_markdown(index=False, floatfmt=".2f")
    
    return f"Resumo dos dados do PRONAF para o estado {cd_estado}:\n\n{tabela}"

# üìú DESCRI√á√ÉO DA FUN√á√ÉO PARA FUNCTION CALLING (TOOLS)
ferramentas = [
    {
        "type": "function",
        "function": {
            "name": "consulta_pronaf_por_estado",
            "description": "Consulta dados agregados do PRONAF por estado (UF)",
            "parameters": {
                "type": "object",
                "properties": {
                    "cd_estado": {
                        "type": "string",
                        "description": "C√≥digo do estado (UF), como 'SP', 'BA', 'RS', etc."
                    }
                },
                "required": ["cd_estado"]
            }
        }
    }
]

# üí¨ CONTROLE DE HIST√ìRICO DE MENSAGENS
if "mensagens" not in st.session_state:
    st.session_state.mensagens = [
        {"role": "system", "content": "Voc√™ √© um assistente que responde com base nos dados do PRONAF brasileiro."}
    ]

# üì• INTERFACE DE ENTRADA DO USU√ÅRIO
pergunta = st.chat_input("Digite sua pergunta sobre os dados do PRONAF...")

# üîÅ ADICIONA A PERGUNTA √Ä CONVERSA E PROCESSA
if pergunta:
    st.session_state.mensagens.append({"role": "user", "content": pergunta})
    
    # ------------------------------------------------------------------
    # Etapa 2: Recuperar o contexto via RAG
    # ------------------------------------------------------------------
    # Aqui usamos a fun√ß√£o que recupera os documentos e os formata
    formatted_context = (vector_db.as_retriever(k=n_documentos) | format_docs).invoke(pergunta)

        # Exibir o contexto recuperado para verifica√ß√£o (para debug)
    # st.markdown("### Contexto Recuperado via RAG:")
    # st.text_area("Contexto", formatted_context, height=300)

    # ------------------------------------------------------------------
    # Etapa 3: Construir o Prompt Combinado
    # ------------------------------------------------------------------
    prompt_template = ( "Voc√™ √© um assistente especializado em cr√©dito rural, com √™nfase nas opera√ß√µes destinadas √† agricultura familiar, "
                        "ou seja, nos dados do PRONAF.\n\n"
                        "Os dados a seguir foram recuperados via RAG\n\n" 
                        "Contexto Recuperado (obtido via RAG):\n"
                        "-----------------------------------------------------------\n" 
                        "{context}\n" 
                        "-----------------------------------------------------------\n\n" 
                        "Pergunta: {question}\n\n" 
                        "Se o contexto n√£o contiver informa√ß√µes suficientes para responder √† pergunta, ou se a consulta exigir"
                        "dados espec√≠ficos de um estado (por exemplo, o c√≥digo de um estado como 'SP', 'RS', etc.), " 
                        "por favor, invoque a fun√ß√£o 'consulta_pronaf_por_estado' para obter um resumo dos dados do PRONAF para o estado em quest√£o. "
                        "Os dados obtidos a partir da fun√ß√£o 'consulta_pronaf_por_estado' foram extra√≠dos da base de dados oficial disponibilizada pelo Banco Central do Brasil"
                        "dispon√≠veis em https://www.bcb.gov.br/estabilidadefinanceira/creditorural."
                        "Com base nos dados oficiais acima e no contexto fornecido, responda utilizando linguagem simples, sendo informativo e proativo."
                        "Caso mesmo ap√≥s a consulta n√£o haja dados suficientes para responder √† pergunta, "
                        "informe que n√£o h√° dados suficientes para elaborar uma resposta"
                         "e solicite, cordialmente, que o usu√°rio aprimore a pergunta." )


    combined_prompt = prompt_template.format(context=formatted_context, question=pergunta)
    
    # Para debug, voc√™ pode exibir o prompt combinado
    # st.markdown("### Prompt Combinado para a LLM:")
    # st.text_area("Prompt", combined_prompt, height=300)
    
    # Imprimi na tela >> Adiciona o prompt combinado √† conversa (como uma mensagem do usu√°rio)
    #st.session_state.mensagens.append({"role": "user", "content": combined_prompt})


    # ------------------------------------------------------------------
    # Etapa 4: Enviar o prompt combinado para a LLM
    # ------------------------------------------------------------------
    # üîç PRIMEIRA CHAMADA PARA VERIFICAR SE A LLM VAI USAR UMA TOOL
    resposta = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=st.session_state.mensagens,
        tools=ferramentas
    )

    mensagem_modelo = resposta.choices[0].message
    tool_calls = mensagem_modelo.tool_calls

    if tool_calls:
        # ‚úÖ A LLM DECIDIU USAR UMA FUN√á√ÉO
        st.session_state.mensagens.append(mensagem_modelo.model_dump())  # Adiciona o pedido de tool_call √† conversa

        # üöÄ EXECUTA A FUN√á√ÉO LOCALMENTE E ADICIONA RESPOSTAS
        for call in tool_calls:
            argumentos = json.loads(call.function.arguments)
            resultado_funcao = consulta_pronaf_por_estado(**argumentos)


            st.session_state.mensagens.append({
                "role": "tool",
                "tool_call_id": call.id,
                "content": resultado_funcao
            })


        # üîÅ SEGUNDA CHAMADA PARA OBTER A RESPOSTA FINAL
        resposta_final = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=st.session_state.mensagens
        )
        mensagem_final = resposta_final.choices[0].message
        st.session_state.mensagens.append({"role": "assistant", "content": mensagem_final.content})
    else:
        # ‚úÖ A LLM RESPONDEU DIRETAMENTE (SEM USAR FUN√á√ÉO)
        st.session_state.mensagens.append({"role": "assistant", "content": mensagem_modelo.content})

# üí¨ EXIBE O HIST√ìRICO DO CHAT
for mensagem in st.session_state.mensagens:
    if mensagem["role"] in ["user", "assistant"]:
        with st.chat_message(mensagem["role"]):
            st.markdown(mensagem["content"])


